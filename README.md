# ScholarAI - Autonomous Research Engineer

<div align="center">

![ScholarAI](https://img.shields.io/badge/ScholarAI-Research%20Assistant-blue?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.11+-green?style=flat-square&logo=python)
![React](https://img.shields.io/badge/React-18+-61DAFB?style=flat-square&logo=react)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-009688?style=flat-square&logo=fastapi)
![TypeScript](https://img.shields.io/badge/TypeScript-5+-3178C6?style=flat-square&logo=typescript)

**An AI-powered research synthesis platform that extracts, analyzes, and synthesizes knowledge from academic documents.**

[Features](#-features) â€¢ [Screenshots](#-screenshots) â€¢ [Quick Start](#-quick-start) â€¢ [How It Works](#-how-it-works) â€¢ [API](#-api-documentation)

</div>

---

## ğŸ¯ What is ScholarAI?

ScholarAI is an **Autonomous Research Engineer** that helps researchers, students, and professionals synthesize knowledge from multiple documents. Unlike chatbots, ScholarAI provides structured research briefs that identify:

- **âœ… Areas of Consensus** - What sources agree on
- **âš¡ Areas of Disagreement** - Conflicting findings and perspectives  
- **â“ Open Questions** - Gaps in the literature that need further research

---

## ğŸ“¸ Screenshots

### Upload & Query Interface
*Enter your research question and upload documents or add URLs*

![Upload Interface](docs/screenshots/01-upload-query.png)

### Processing Pipeline
*Watch real-time progress as documents are processed through the 4-stage pipeline*

![Processing](docs/screenshots/02-processing.png)

### Sources Panel
*View all processed sources with metadata and extraction statistics*

![Sources](docs/screenshots/03-sources.png)

### Claims Extraction
*Explore extracted claims categorized by consensus level*

![Claims](docs/screenshots/04-claims.png)

### Research Brief
*Comprehensive synthesis with consensus, disagreements, and open questions*

![Research Brief](docs/screenshots/05-research-brief.png)

### How It Works Guide
*Built-in documentation explaining the RAG pipeline and technical concepts*

![How It Works](docs/screenshots/06-how-it-works.png)

---

## ğŸ”„ How It Works

### RAG Pipeline Architecture

```mermaid
flowchart TB
    subgraph INPUT ["ğŸ“¥ Input Layer"]
        A[ğŸ“„ PDF/DOCX/PPTX] 
        B[ğŸ”— URLs]
        C[â“ Research Query]
    end
    
    subgraph PROCESSING ["âš™ï¸ Document Processing"]
        D[ğŸ”§ Docling Converter]
        E[ğŸ“ Markdown/JSON Output]
        F[âœ‚ï¸ Semantic Chunking]
    end
    
    subgraph VECTORIZATION ["ğŸ§® Vectorization"]
        G[ğŸ¤– Sentence Transformer]
        H[ğŸ“Š 384-dim Embeddings]
        I[(ğŸ—„ï¸ ChromaDB)]
    end
    
    subgraph RETRIEVAL ["ğŸ” Retrieval"]
        J[ğŸ¯ Query Embedding]
        K[ğŸ“ˆ Similarity Search]
        L[ğŸ”€ MMR Re-ranking]
    end
    
    subgraph SYNTHESIS ["ğŸ§  LLM Synthesis"]
        M[ğŸ“‹ Claim Extraction]
        N[ğŸ·ï¸ Classification]
        O[ğŸ“Š Brief Generation]
    end
    
    subgraph OUTPUT ["ğŸ“¤ Output"]
        P[âœ… Consensus]
        Q[âš¡ Disagreements]
        R[â“ Open Questions]
    end
    
    A --> D
    B --> D
    D --> E --> F
    F --> G --> H --> I
    C --> J --> K
    I --> K --> L
    L --> M --> N --> O
    O --> P & Q & R
```

### Pipeline Stages Explained

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ScholarAI Processing Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚    1     â”‚    â”‚    2     â”‚    â”‚    3     â”‚    â”‚    4     â”‚              â”‚
â”‚  â”‚Processingâ”‚â”€â”€â”€â–¶â”‚Retrievingâ”‚â”€â”€â”€â–¶â”‚Extractingâ”‚â”€â”€â”€â–¶â”‚Synthesizeâ”‚              â”‚
â”‚  â”‚          â”‚    â”‚          â”‚    â”‚          â”‚    â”‚          â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚       â”‚               â”‚               â”‚               â”‚                     â”‚
â”‚       â–¼               â–¼               â–¼               â–¼                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ â€¢ Doclingâ”‚    â”‚ â€¢ Embed  â”‚    â”‚ â€¢ GPT-4  â”‚    â”‚ â€¢ Format â”‚              â”‚
â”‚  â”‚ â€¢ Chunk  â”‚    â”‚   Query  â”‚    â”‚ â€¢ Extractâ”‚    â”‚ â€¢ Score  â”‚              â”‚
â”‚  â”‚ â€¢ Embed  â”‚    â”‚ â€¢ Vector â”‚    â”‚   Claims â”‚    â”‚ â€¢ Export â”‚              â”‚
â”‚  â”‚ â€¢ Index  â”‚    â”‚   Search â”‚    â”‚ â€¢ Classifyâ”‚   â”‚          â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Claim Classification System

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        Extracted Claims             â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼               â–¼               â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  âœ… CONSENSUS â”‚ â”‚âš¡ DISAGREEMENTâ”‚ â”‚ â“ UNCERTAIN â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚ Multiple    â”‚ â”‚ Sources     â”‚ â”‚ Insufficientâ”‚
            â”‚ sources     â”‚ â”‚ conflict    â”‚ â”‚ evidence or â”‚
            â”‚ agree       â”‚ â”‚ on findings â”‚ â”‚ single      â”‚
            â”‚             â”‚ â”‚             â”‚ â”‚ source      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Features

### Document Processing
- ğŸ“„ **Multi-format support**: PDF, DOCX, PPTX, images (PNG, JPG)
- ğŸ”— **URL processing**: Academic papers, web articles, arXiv links
- ğŸ§  **Intelligent chunking**: Semantic boundaries with configurable overlap
- ğŸ“Š **Metadata extraction**: Authors, dates, titles automatically detected

### RAG Pipeline
- ğŸ” **Semantic search**: ChromaDB vector store with sentence transformers
- ğŸ¯ **Query expansion**: Automatic synonym and concept expansion
- ğŸ“ˆ **Relevance scoring**: MMR-based re-ranking for diverse results

### Claim Extraction
- âœ… **Consensus detection**: Claims supported by multiple sources
- âš¡ **Disagreement identification**: Conflicting viewpoints highlighted
- â“ **Uncertainty flagging**: Areas needing more research
- ğŸ”— **Source attribution**: Every claim linked to original sources

### Research Brief Synthesis
- ğŸ“‹ **Structured output**: Organized sections for easy navigation
- ğŸ“Š **Confidence scoring**: Overall reliability assessment
- âš ï¸ **Limitations noted**: Transparent about research gaps
- ğŸ“¤ **Export ready**: Copy-to-clipboard functionality

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.11+** with pip
- **Node.js 18+** with npm
- **OpenAI API Key** (for claim extraction and synthesis)

### 1. Clone the Repository

```bash
git clone https://github.com/sparsh2005/ScholarAI.git
cd ScholarAI
```

### 2. Backend Setup

```bash
# Navigate to backend
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Create environment file
echo "OPENAI_API_KEY=your-api-key-here" > .env

# Start the backend server
uvicorn main:app --reload --port 8000
```

### 3. Frontend Setup

```bash
# From project root
cd ..  # if you're in backend/

# Install dependencies
npm install

# Start the development server
npm run dev
```

### 4. Open the Application

Visit **http://localhost:8080** in your browser.

---

## ğŸ—ï¸ Architecture

### System Overview

```mermaid
graph TB
    subgraph Frontend ["ğŸ–¥ï¸ Frontend (React + TypeScript)"]
        UI[UI Components]
        State[Zustand Store]
        API[API Client]
    end
    
    subgraph Backend ["âš™ï¸ Backend (FastAPI + Python)"]
        Routes[API Routes]
        Services[Services Layer]
        Models[Pydantic Models]
    end
    
    subgraph Services ["ğŸ”§ Core Services"]
        Docling[Docling Service]
        Embed[Embedding Service]
        Vector[Vector Store]
        Claims[Claim Extractor]
        Synth[Synthesizer]
    end
    
    subgraph External ["ğŸŒ External"]
        OpenAI[OpenAI GPT-4]
        Chroma[(ChromaDB)]
    end
    
    UI --> State --> API
    API --> Routes --> Services --> Models
    Services --> Docling & Embed & Vector & Claims & Synth
    Claims & Synth --> OpenAI
    Vector --> Chroma
```

### Project Structure

```
ScholarAI/
â”œâ”€â”€ backend/                 # FastAPI Backend
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes/          # API endpoint handlers
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py       # Pydantic models
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ docling_service.py      # Document processing
â”‚   â”‚   â”œâ”€â”€ embedding_service.py    # Text embeddings
â”‚   â”‚   â”œâ”€â”€ vector_store.py         # ChromaDB operations
â”‚   â”‚   â”œâ”€â”€ claim_extractor.py      # LLM claim extraction
â”‚   â”‚   â””â”€â”€ synthesizer.py          # Brief generation
â”‚   â”œâ”€â”€ tests/               # Pytest test suite
â”‚   â”œâ”€â”€ config.py            # Settings management
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ src/                     # React Frontend
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ brief/           # Research brief display
â”‚   â”‚   â”œâ”€â”€ claims/          # Claims panel
â”‚   â”‚   â”œâ”€â”€ sources/         # Sources panel
â”‚   â”‚   â”œâ”€â”€ upload/          # Upload components
â”‚   â”‚   â”œâ”€â”€ layout/          # App layout
â”‚   â”‚   â””â”€â”€ ui/              # shadcn/ui components
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â””â”€â”€ use-research.ts  # Zustand state management
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â””â”€â”€ api.ts           # API client
â”‚   â””â”€â”€ pages/               # Route pages
â”‚
â”œâ”€â”€ examples/                # Sample documents & outputs
â””â”€â”€ public/                  # Static assets
```

### Technology Stack

| Layer | Technologies |
|-------|-------------|
| **Frontend** | React 18, TypeScript, Tailwind CSS, shadcn/ui, Zustand |
| **Backend** | FastAPI, Python 3.11+, Pydantic v2 |
| **Document Processing** | Docling (IBM Research), pypdf |
| **Vector Store** | ChromaDB, sentence-transformers (all-MiniLM-L6-v2) |
| **LLM** | OpenAI GPT-4 (claim extraction, synthesis) |

---

## ğŸ”§ API Documentation

### Base URL
```
http://localhost:8000
```

### Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/health` | Health check |
| `POST` | `/api/process-docs/upload` | Upload a document |
| `POST` | `/api/process-docs` | Process documents & URLs |
| `POST` | `/api/retrieve-chunks` | Semantic search |
| `POST` | `/api/extract-claims` | Extract & classify claims |
| `POST` | `/api/synthesize-report` | Generate research brief |
| `GET` | `/api/storage-stats` | Get storage usage |
| `DELETE` | `/api/clear-data` | Clear all stored data |

### Interactive API Docs

Visit **http://localhost:8000/docs** for Swagger UI documentation.

---

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
pytest tests/ -v --cov=.

# Frontend tests
npm run test
```

---

## âš™ï¸ Configuration

### Backend Environment Variables

Create `backend/.env`:

```env
# Required
OPENAI_API_KEY=sk-...

# Optional - Defaults shown
UPLOAD_DIRECTORY=./data/uploads
PROCESSED_DIRECTORY=./data/processed
CHROMA_PERSIST_DIRECTORY=./data/chroma
CHUNK_SIZE=512
CHUNK_OVERLAP=50
EMBEDDING_MODEL=all-MiniLM-L6-v2
MAX_FILE_SIZE_MB=50
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Docling](https://github.com/DS4SD/docling) - Document processing by IBM Research
- [ChromaDB](https://www.trychroma.com/) - Vector database
- [shadcn/ui](https://ui.shadcn.com/) - UI components
- [FastAPI](https://fastapi.tiangolo.com/) - Backend framework

---

<div align="center">

**Built with â¤ï¸ for researchers everywhere**

[â¬† Back to Top](#scholarai---autonomous-research-engineer)

</div>
